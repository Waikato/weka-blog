<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>WEKA Blog (Posts by Eibe Frank)</title><link>https://waikato.github.io/weka-blog/</link><description></description><atom:link href="https://waikato.github.io/weka-blog/authors/eibe-frank.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 04 Jul 2020 07:50:52 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>New WekaDeeplearning4j Release - Pretrained Models, Feature Extraction Update, and more</title><link>https://waikato.github.io/weka-blog/posts/2020-07-04-wekaDeeplearning4j-1.6.0/</link><dc:creator>Eibe Frank</dc:creator><description>&lt;div&gt;&lt;div class="section" id="pretrained-models"&gt;
&lt;h2&gt;Pretrained Models&lt;/h2&gt;
&lt;p&gt;The package previously already contained a model zoo --- a set of model architectures designed by others (e.g., AlexNet, ResNet50) --- but there was no easy way to use a pretrained checkpoint in Wdl4jMlpClassifier so training these models had to be done from scratch. The new release of WekaDeeplearning4j both expands the model zoo to contain over 30 models and provides an easy way to initialize them with pre-trained weights that have been made publicly available for these models. This makes it even easier to start playing with state-of-the-art neural networks (e.g., EfficientNet): not only do you not need programming experience --- due to the WEKA GUI --- but now you don't even need a beefy GPU to start getting useful results because these models can simply be used as feature extractors without any extra training, simply using the pre-trained weights.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="updated-dl4jmlpfilter"&gt;
&lt;h2&gt;Updated Dl4jMlpFilter&lt;/h2&gt;
&lt;p&gt;This filter allows you to use a model --- which can now be a pretrained model from the Model Zoo --- as a feature extractor, converting an image dataset into a numeric form that can be used with any off-the-shelf WEKA classifier. In the new version, the filter also supports multiple feature extraction layers: by default the last dense layer will be used, but you can alternatively choose any intermediary layer as well, concatenating the activations from the two layers. This opens up a huge new world of experimentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="image-dataset-conversion-script"&gt;
&lt;h2&gt;Image Dataset Conversion Script&lt;/h2&gt;
&lt;p&gt;Some image classification datasets come in a simple 'folder-organised' fashion, where collections of images are split into subfolders, with the name of each subfolder providing the class of all images within it. To make it easier to work with these types of image datasets, the package now includes the &lt;cite&gt;ImageDirectoryLoader&lt;/cite&gt; which can load in datasets of this form.&lt;/p&gt;
&lt;p&gt;Check out the &lt;a class="reference external" href="https://deeplearning.cms.waikato.ac.nz"&gt;documentation&lt;/a&gt; for more info on these new features!&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>github</category><guid>https://waikato.github.io/weka-blog/posts/2020-07-04-wekaDeeplearning4j-1.6.0/</guid><pubDate>Sat, 04 Jul 2020 06:06:00 GMT</pubDate></item><item><title>Micro averages in multi-class classification</title><link>https://waikato.github.io/weka-blog/posts/2019-02-16-micro_average/</link><dc:creator>Eibe Frank</dc:creator><description>&lt;div&gt;&lt;p&gt;When evaluating multi-class classification models, Weka outputs a weighted average of the per-class precision, recall, and F-measure: it computes these statistics for each class individually, treating the corresponding class as the "positive" class and the union of the other classes as the negative class, and computes a weighted average of these per-class statistics, with a per-class weight that is equal to the proportion of data in that class. A recent question on the Weka mailing list was whether the software also outputs micro-averaged precision, recall, and F-measure for multi-class problems. It turns out that it does! To find out where these can be found, read on.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://waikato.github.io/weka-blog/posts/2019-02-16-micro_average/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>github</category><guid>https://waikato.github.io/weka-blog/posts/2019-02-16-micro_average/</guid><pubDate>Sat, 16 Feb 2019 01:48:00 GMT</pubDate></item><item><title>Oversampling and Undersampling</title><link>https://waikato.github.io/weka-blog/posts/2019-01-30-sampling/</link><dc:creator>Eibe Frank</dc:creator><description>&lt;div&gt;&lt;p&gt;A frequent question of Weka users is how to implement oversampling or undersampling, which are two common strategies for dealing with imbalanced classes in classification problems. This post provides some explanation.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://waikato.github.io/weka-blog/posts/2019-01-30-sampling/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>github</category><guid>https://waikato.github.io/weka-blog/posts/2019-01-30-sampling/</guid><pubDate>Tue, 29 Jan 2019 23:10:00 GMT</pubDate></item></channel></rss>